# 论文阅读：

## Abstract

由于用户行为的不确定性和优先的信息，在基于网站的行为模型研究中，基于匿名session来预测用户的行为是个挑战。最近先进的使用递归神经网络来解决这个问题有一定的前景，使用LSTM模型很有效的能从==之前==的点击中捕捉到用户普遍的兴趣。但是，没有方法能显式地将用户==当前==的动作对下一个动作的影响考虑其中。

我们将讨论长期记忆的模型不足以来对长期的sessions建模，因为里面含有用户兴趣的迁移。

一个短期的基于attention/memory 优先的模型被提出作为补救。它能够从Session上下文的长期记忆中捕获用户的一般兴趣.whilst 考虑用户来自最后一次点击的短期记忆的当前的兴趣。

## 关键词

行为建模；基于Session的推荐；注意力机制；表示学习；神经网络



1 一种是 用户只是浏览了商品的描述，没有做出购买的决定，那么用户很可能继续浏览其他数码相机

2 用户将数码相机加入购物车，那么接下来用户很可能回去购买周边产品比如内存卡。



SRS session_based recommender Systems

长期兴趣和短期兴趣都非常重要，



近期行为优先机制：STAMP short term Attention/Memory priority

这可以同时考虑到用户的总体兴趣和他/她当前的兴趣。





捕获用户全局的兴趣方向：

1.基于用户的全部购买/点击历史记录，使用协同过滤算法

比如：使用MF，来分解全部历史记录的用户-商品矩阵

2.近邻模型，从Sesson中找共现商品，算商品的相似度，找近邻

3.马尔科夫链，利用用户行为间序列来预测。



SWIWO

学习用户Session的上下文。

它通过合并当前会话中的所有项的嵌入 来构造会话上下文，根据与目标项的相对距离为每个项赋予一个固定的权重

NARM 

它将RNN中的最后一个隐藏状态作为顺序行为，并使用之前单击的隐藏状态进行注意力计算，以捕获给定会话中的主要目的(一般兴趣)



S = [s1, s2, . . . , sN],包含着一个序列的行为（被用户点击的商品）

s_i 代表一个商品（ID）在第i个时间被点击

S_t {s_1     s_t} 表示一个动作序列在时间t被截断的前缀

V = {v_1,v_2     v_|V|} 是一个 set集合  唯一标志符（商品目录）

X = {X_i}  是商品的  嵌入向量

STAMP 模型 学习 一个 d 维的 真实值的嵌入。

每个商品都用一个 d 维得向量表示。

x_t 是当前会话的前缀St的最后一次点击s_t的嵌入（embedding）

我们的目标是预测s_t+! 可能点击的 商品



准确地说，我们的模型被构造和训练成一个分类器，学习为条目字典V中的每个候选项生成一个分数，

y_hat={y_1,y_2  .....y_|V|}

每个y_i代表项目v_i的预测分数。

然后对 预测得到的y_hat 进行降序排列，

topk就是我们需要的



Hadamard product, **Hadamard**乘法

两个向量b和c之间的元素乘积



STMP 模型 包含两个嵌入的向量作为输入

m_s 表示用户对当前会话的兴趣，它被定义为会话的external memory的**平均值**



### STMP模型

这里的external memory指的是嵌入当前会话前缀St的项

![1554355536326](E:\剑指Offer\notebook\1554355536326.png)

m_t 表示该会话中的current的兴趣（the last  click）

都被送入MLP 网络中 进行特征提取

两个MLP模型是相同的，除了它们有独立的参数设置。

使用一个简单的没有隐藏层的MLP进行特征提取，对ms的操作定义为


$$
h_s = f(W_s m_s+b_s)\\
W_s ∈ R^{d×d} is~ a~  weighting~~ matrix \\
b_s 偏置项向量 \\
f(.) 表示非线性激活 此处用的tanh
$$
$h_t$采用同样的方法计算得到。

给出一个商品的项目$X_i \in   V$ 

最终的得分定义为：
$$
\hat{z_i} = \sigma(<h_s,h_t,x_i>) \\
\sigma 表示sigomid函数 \\
\hat{z} \in R^{|V|} 表示非标准化 的余弦相似度
$$
加权用户兴趣表示的当前Session 的前St项和候选项习之间的相似性

再通过softmax函数获取输出：
$$
\hat{y} = softamx(\hat{z}) \\
y_i 表示的是v_i物品可能在下次被点击的事件的概率
$$
损失函数被定义为交叉熵函数：
$$
L(\hat{y}) = -\sum_{i = 1}^{|V|}y_ilog(\hat{y_i})-(1-y_i)log(1-\hat{y_i})
$$
使用随机梯度下降算法优化交叉熵损失。

该算法缺点：

STMP模型从Session中提取数据时候 是平均对待每一个商品。这在捕获用户的兴趣迁移的时候是有问题的，可能是由于无意的点击，捕获到噪声。特别是在长会话中。因此提出了一个attention的模型来解决这个问题。

### STAMP Model

![1554358234373](E:\剑指Offer\notebook\1554358234373.png)

$h_s$的计算从基于用户兴趣Attention Net 产生而来

#### Attention Net

两个组件：

1. 一个简单的前馈神经网络FNN，用来生成attention当前会话前缀St中的每个项的权重值
2. 一种注意力合成函数，负责计算基于用户的总体的兴趣的注意力

FNN的计算方式：
$$
a_i = W_o\sigma(W_1x_i+W_2x_t+W_3m_s+b_a) \\
x_i \in R^d表示第i个项目s_i\in S_t \\
x_t \in R^d 表示最后一次点击的商品 \\
W_0 \in R^{1×d} 权重向量, \\
W_1,W_2,W_3 \in R^{d×d} 权重矩阵 \\
b_a \in R^d 偏置项向量\\
\sigma() 表示sigmoid 函数 \\
最终求出的a_i表示物品x_i的attention 系数
$$
目标商品attention系数的计算

来自于目标商品x_i，最后一次的点击x_t 以及Session的平均代表m_s。

因此，它能够捕获目标项与用户兴趣的长期/短期内存之间的关联。
$$
那么会话前缀S_t的总体兴趣可以计算表示为： \\
m_a = \sum_{i=1}^{t}a_ix_i
$$

### STMO

the short-Term Memory Only model

只基于用户最后一次的点击s_t，当前会话前缀s_t的预测模型。
$$
h_t = f(W_tx_t+b_t) \\
\hat{z_i} = h_t^Tx_i \\
$$
就是STMP模型中的MLP CELL B

获取到分数向量z之后，通过softmax计算预测的最大概率

然后通过交叉熵函数和SGD进行优化。



